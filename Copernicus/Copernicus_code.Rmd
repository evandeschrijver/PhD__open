---
title: "Copernicus_extraction_process"
author: "Evan de Schrijver"
date: "6/16/2022"
output: html_document
---

```{r setup, warning=FALSE, message=FALSE, eval=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(ecmwfr); library(RNetCDF); library(rgdal); library(raster); library(ncdf4)
library(datetime); library(maptools); library(lubridate) ; library(tidyverse) ; library(rnaturalearth); library(RColorBrewer); library(viridis); library(maps); library(mapdata); library(ncdf4); library(tictoc); library(exactextractr); library(sf); library(reshape2)


```
<br>
<br>

### How to extract data from the Coperncius server and how to develop it into a daily Time series format which is useful for any environmental epidemiology study.

<br>
The C3S Climate Data Store (CDS) is a one-stop shop for information about the climate: past, present and future. It provides easy access to a wide range of climate datasets via a searchable catalogue. An online toolbox is available that allows users to build workflows and applications suited to their needs. Most of the time, we can extract variables such as temperature, humidity amongst others at a high resolution. It depends on the dataset, however, the most common ones are: ERA5 (30km global resolution), ERA5-land (6km resolution, also global coverage) and CMIP5 and CMIP6 for climate projections. We can approach this in two ways:
<br>

 1. We go directly to the CDS and use buttons to press 
 2. We access it using an API-key in R. 
<br>


The aim of this tutorial is to learn:
<br>

* how to extract data from the Copernicus server
* how to extract the data from the netcdf file
* how to convert the raster data in time series
* How to link temperature and mortality data read to be analysed!

<br>
<br>

In this example we use historical data from the CMIP6 climate projections as the temperature exposure from the CMIP6 General Circulation Model: "Access_cm2"

<br>
First we go to the Copernicus Climate Data store:
https://cds.climate.copernicus.eu/user/login?destination=%2F%23!%2Fhome
<br>
Here we first create our own account. 
<br>

1. Once you have created your own account: log in

2. Press on your name right top 

3. Scroll down and find your "UID" = (5 numbers) and "API Key", you will need them in the code later.

<br>

So now we have an account and we can directly access the Coperncius Data Stroage (CDS) from R! This has several advantges:
<br>

* You can download more data at once
* Reproducibility 
* Run several request after each other without having to manually do each one of them (Usefull for when downloading big data)
* With large data you do not have to import it first it into your own computer before storing it on a server

<br>

### 1) Download data from Copernicus and save it 

#### The code is provided below and I quickly explain what we do in each step (Steps are also indicated in the code chunck):
<br>

1. We fill out our own user="UID" and key="API key" which we derived earlier. mine are: user ="xxxxx", key = "xxxxxx-xxxxxxx-xxxxxxxx-xxxxxx". This is needed for the package EMCWFR. This allows us to access the CDS.
<br>

2. In step 2 we set the options of the data extraction (i.e. what model we want to extract the data from or what the global coverage should be. We sinply extract the global data here but you can reduce the size of the bounding box, which of course speeds up the download)
<br>

3. This is where it gets more complex. Depending on the dataset you download data from (i.e. ERA5, CMIP5, CMIP6 etc.), they all have a slightly different set up as a list to pull the request. To make it easy do the following.
 <br>
 
  * go to your dataset of interest in CDS
  * go to download data and press the buttons: What sort of variable you want to extract, select some years, month, day, hours 
  * Scroll down and press "whole available region"
  * Select with format: "Compressed tar file (.tar.gz)". NOTE: some datasets only have NETCDF then take that one 
  * Show API request 
  * Copy paste the code bewlow and paste it into R.
  * Select from : "c.retrieve" onwards.
  * While selecting the copied/pasted code, go to "Addins" in R (on the top) 
  * Press Python to list
  * Well done, it has been converted to something R can work with :) 
  <br>
  
  * Now we can play round with the variables --> change the years, time, space you want to extract your data from
  
<br>
  
4. We pull the request with all the information set above (Do not forget to update your own UID number)

5. We create a lists of all the requests generated

6. We pull all the requests into a list and extract them all at once

7. This step is only required if you have a tar.gz. file. If you have a netcdf file, skip this step.

<br>

NOTE: depending on how often you submit a request, time of the day, and size of the request --> it can take up to one or two hours before it start down loading, this is because you are in a cue to extract data. When waiting time exceeds two hours in R you are being kicked out. However Copernicus has not forgotten your request. Simply rerun to step 6 and run that line again. Most probably you can download it immediately. If it still is loading and not downloading with a percentage bar shown, simply wait --> it can take a while because it is probably busy :) 

<br>


```{r, eval = FALSE, echo=TRUE,warning = FALSE, message = FALSE}
# Set working directory and where you want to store your data 
setwd("~/Desktop/Github/PhD_open/Copernicus")
dir <- "~/Desktop/Github/PhD_open/Copernicus"

# 1) Inlcude personal login data for API key
  wf_set_key(user ="xxxx", key = "xxxx-xxxx-xxxxx-xxxxxx", service = "cds")

# 2) put in general informaiton of data extraction

  # model
  model <- "acces_cm2"
  # global coordinates of the raster data you want coverage
  Nc <- 90
  Wc <- -180
  Sc <- -90
  Ec <- 180

#This can be useful when looping requests
  cmip_model <- model
  region_name <- "world"
  cat("",cmip_model)


# 3)We make the request
request1 <- list(
  temporal_resolution = "daily",
  experiment = "historical",
  level = "single_levels",
  variable = "near_surface_air_temperature",
  model = paste0(cmip_model),
  area = c(Nc,Wc,Sc,Ec), #N,W,S,E
  format = "tgz",
  dataset_short_name = "projections-cmip6",
  target = "historical.tar.gz"
)

# 4) We pull the request (Do not forget to update your own UID number)
download_pull <- function(x) {
  wf_request(user = "xxxxx",
             request = x,  
             transfer = TRUE, 
             path = paste0(dir,cmip_model))
  
}


# 5) CREATE LIST OF REQUESTS
  request_list <- list(request1)

#6 ) WE EXTRACT THE DATA DOWNLOAD IT
  walk(request_list, download_pull)
  
#7 ) IF YOU DONWLOADED TAR FILE, THIS IS NECESSARY. IF DOWNLOADED A NETCDF FILE: SKIP THIS
  untar(tarfile = "historical.tar.gz", exdir = "GFDL_historical")



```
<br>
<br>
<br>
<br>

### 2) Import and NETCDF data

So here we import the raster file using the brick function from the raster package. We already select the Variable name "tas" from the brick, which is the temperature data in this case. We then print the climatevariable to see what the characteristics are of our imported raster brick. It is a brick so it has 3 dimensions: Longtitude, Latitude and height which represents the number of days.
<br>
Some useful information is provided by this summary. For example:
dimensions: the number of rows (latitudes), columns (longitudes), total pixels (rows multiplied by columns), and time layers (dates) of the file.
resolution: shows the resolution (or “pixel size”) of the data (in degrees in this case). A pixel of 1.875°x1.25° means that each pixel represents an area of approximately 200km x 140 km of the earth's surface.
extent: shows the spatial coverage (that is, the maximum and minimum latitude and longitude limits) of the NetCDF file.
crs: the coordinate system in which the file is projected. This file is in lat / lon coordinate system with the WGS84 datum.
date: the temporal coverage of the data, in this case going from 10.13.1973 (the first layer) to 15.07.1998 (the last layer).
```{r, message=FALSE,warning = FALSE}
library(ecmwfr); library(RNetCDF); library(rgdal); library(raster); library(ncdf4)
library(datetime); library(maptools); library(lubridate) ; library(tidyverse) ; library(rnaturalearth); library(RColorBrewer); library(viridis)
library(maps); library(mapdata); library(ncdf4); library(tictoc); library(exactextractr); library(sf); library(reshape2)

#Readnetcdf   using the raster brick function --> in varnamne specific the variable name you would like to extract (in this case it is tas for daily mean temperature)
  climatevar <- brick("temp_data/tas_day_ACCESS-CM2_historical_r1i1p1f1_gn_19731013-19980715_v20191108.nc",varname="tas",stopIfNotEqualSpaced=FALSE)
#short_series <- subset(climatevar,1:1000)




# CHECK INFORMATION RASTER BRICK
# The dimensions are given too: 144 rows, 192 columns (raster dimension=LON*LAT which represents on slice of the raster brick layer), the equivalent of 27648 cells per day! and 9042 days (height of the cube

  climatevar
#Dimensions raster brick --> 144 cells * 192 cells * 9042 our raster has.
  dim(climatevar)
```

Now Lets plot it, once slice only! 1st of January 1980
```{r, message=FALSE,warning = FALSE}
# LETS PLOT THEM FOR A day
  my.palette <- colorRampPalette(brewer.pal(9,"RdBu"))(100)
  plot(climatevar$X1980.01.01,col=rev(my.palette))
```
<br>
Lets go from Kelvin to degrees Celsiuis
```{r, message=FALSE,warning = FALSE}
# LETS PLOT THEM FOR A day
  climatevar_c <- climatevar-273.15
  plot(climatevar_c$X1980.01.01,col=rev(my.palette))
```
<br>
<br>


### 3) Extract NETCDF data

Now let's extract our data and actually start processing it. We only have our raster data imported and ready to analyse. So now we also need a shapefile to extract the raster data from in locations where they overlap. <br>
So we do the following here: 
<br>

1. We import the shapefile with the adequate districts
2. We import the raster brick with temperature data 
3. We extract the raster data using the "exact_extract" function.
4. We melt the extracted data from wide to long format (which is preferred for time series)
5. We create year and date variables 
6. We convert the data from Kelvin to degrees Celsius. 

Here we use the exactextractr package/function. It is an R package that quickly and accurately summarizes raster values over polygonal areas, commonly referred to as zonal statistics. Unlike most zonal statistics implementations, it handles grid cells that are partially covered by a polygon and therefore you can more accurately get the temperature series compared to many other extraction functions. the Packages typical performance for real-world applications and is orders of magnitude faster than the raster package.

```{r, message=FALSE,warning = FALSE}
# set data directory
  CMIP5.proj <- "~/Desktop/Github/Phd_open/Copernicus/temp_data"
#Set the colour for a plot
  my.palette <- colorRampPalette(brewer.pal(9,"RdBu"))(100)
  

#  1 load the shapefile 
     locations.sp  <- st_read("shapefile/district.shp")

# 2 load the raseterbirck 
      ##Get all the nc files within the CLIMM folder
      CLIMM.NC.files <- list.files(paste0(CMIP5.proj))
      
      #It has multiple folders because it was too big to download all at once
      CLIMM.NC.files
      

      ###load the nc file
      CLIMM.SSPRCP.NCfilek <- CLIMM.NC.files
      CLIMM.SSPRCP.NCfilek.brick <- brick(paste0(CMIP5.proj,"/",CLIMM.SSPRCP.NCfilek), varname = "tas")
          
      summary(CLIMM.SSPRCP.NCfilek.brick$X1980.01.01) ##tas (Temperature) is in Kelvin --> we will deal with that in a later step
          
          
#  3. We extract the raster data using the "exact_extract" function.     
          
       # Finally extracting the raster data based on the polygons!
      #Apologies for not being able to delete the loading bar :/ 
          
      tic()
      CLIMM.SSPRCP.NCfilek.daily <- exact_extract(CLIMM.SSPRCP.NCfilek.brick, locations.sp, fun="mean")
          
      # create a common ID number nad name and bind the days to the locations
      ID <-1:nrow(locations.sp)
      NAME <- locations.sp$NAME
      CLIMM.SSPRCP.NCfilek.daily <- cbind(ID,NAME,CLIMM.SSPRCP.NCfilek.daily)
      toc()

```
<br>
<br>
Amazing now we have extracted the data for each day for each polygon! Now let's see what that look like!
```{r, message=FALSE,warning = FALSE}
#plot first 5 days with ID and name of district
    head(CLIMM.SSPRCP.NCfilek.daily[,1:7])
```

<br>
<br>
Slight issue, it is not time series format yet! Lets convert it into long format so we can actually work with it.
```{r, message=FALSE,warning = FALSE}
# 4. We melt the extracted data from wide to long format (which is preferred for time series)        
#Melt the dataframe to get the Date variable and Tmean.C 

        CLIMM.SSPRCP.NCfilek.daily.df <- melt(id.vars=c("ID","NAME") ,CLIMM.SSPRCP.NCfilek.daily,variable.name="Date", value.name="Tmean.C")
        colnames(CLIMM.SSPRCP.NCfilek.daily.df) <- c("ID","NAME","Date","Tmean.C")
        head(CLIMM.SSPRCP.NCfilek.daily.df)
```

<br>
<br>
Now the dates are still an issue. Lets use the gsub function to solve this. Lets also create an indicator for the  year and convert it all from Kelvin to degrees Celsuis (This is quicker  in a dataframe rather than doing it earlier in the raster itself)
```{r, message=FALSE,warning = FALSE}
# 5. We create year and date variables 
      CLIMM.SSPRCP.NCfilek.daily.df$Date <- as.Date(gsub("mean.X", "", as.character(CLIMM.SSPRCP.NCfilek.daily.df$Date)), format=c("%Y.%m.%d"))
      head(CLIMM.SSPRCP.NCfilek.daily.df) 
          
 # NOW ALSO CREATE A YEAR VARIABLE
      CLIMM.SSPRCP.NCfilek.daily.df$year <- year(CLIMM.SSPRCP.NCfilek.daily.df$Date)

          
# 6. We convert the data from Kelvin to degrees celsius.                     
    
 # FROM KELIVIN TO TMEAN.C --> I do it in the end because this is quicker than running it over raster
      CLIMM.SSPRCP.NCfilek.daily.df$Tmean.C <- CLIMM.SSPRCP.NCfilek.daily.df$Tmean.C - 273.15
            
      head(CLIMM.SSPRCP.NCfilek.daily.df)
  
```

<br>
<br>
We are finally there! We now have the ID, NAME, Date and temperature in degrees Celsius with an indicator for year in long format! But lets make it Tidy using the 'tidyr'package. These consist of tools to help create tidy data, where each column is a variable, each row is an observation, and each cell contains a single value. 'tidyr' contains tools for changing the shape (pivoting) and hierarchy (nesting and 'unnesting') of a dataset
```{r, message=FALSE,warning = FALSE}
    ## create tiblle with nested list of TS per municpiality --> maybe tidyr way of storing the data
    CLIMM.SSPRCP.NCfilek.daily.list <- CLIMM.SSPRCP.NCfilek.daily.df %>% 
                                        select(ID,NAME,Date,Tmean.C,year) %>% 
                                        nest(TS=c(Date,year,Tmean.C))
    
    #Now we have a list dataset by district in a dataframe! 
    head(CLIMM.SSPRCP.NCfilek.daily.list)
    
    #Lets see what that looks like for the first district:
    head(CLIMM.SSPRCP.NCfilek.daily.list$TS[[3]])
    
    # plot time series for first district
p <- ggplot(CLIMM.SSPRCP.NCfilek.daily.list$TS[[3]], aes(x=Date, y=Tmean.C)) +
  geom_line() + theme_bw()+
  xlab("")
p


    
    #save the results
    saveRDS(CLIMM.SSPRCP.NCfilek.daily.list, paste0(dir,"/GFDL_histroical_1973_1998.rds"))

```